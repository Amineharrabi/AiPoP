name: Incremental Data Update

on:
  schedule:
    - cron: "0 */6 * * *" # Run every 6 hours (more reasonable than 10 min)
  workflow_dispatch: # Allow manual trigger

# Prevent concurrent runs from conflicting
concurrency:
  group: data-update
  cancel-in-progress: false

jobs:
  update-data:
    runs-on: ubuntu-latest
    environment: production
    permissions:
      contents: write # Needed for pushing to the repository
      issues: write # Needed for creating issues on failure

    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0 # Need full history for git operations
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download existing database
        continue-on-error: true
        run: |
          # Try to download existing database from latest release or artifacts
          if [ -f "data/warehouse/ai_bubble.duckdb" ]; then
            echo "Database already exists"
          else
            echo "No existing database found - will need full setup"
          fi

      - name: Set up environment variables
        run: |
          echo "PYTHONUNBUFFERED=1" >> $GITHUB_ENV
        env:
          SEC_API_KEY: ${{ secrets.SEC_API_KEY }}
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
          GITHUB_TOKEN: ${{ secrets.GH_PAT }}
          HF_ACCESS_TOKEN: ${{ secrets.HF_ACCESS_TOKEN }}
          NEWS_API_KEY: ${{ secrets.NEWS_API_KEY }}

      - name: Initialize database if needed
        run: |
          if [ ! -f "data/warehouse/ai_bubble.duckdb" ]; then
            echo "Initializing database..."
            python setup/setup_duckdb.py
          else
            echo "Database exists, skipping initialization"
          fi
        env:
          SEC_API_KEY: ${{ secrets.SEC_API_KEY }}
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
          GITHUB_TOKEN: ${{ secrets.GH_PAT }}
          HF_ACCESS_TOKEN: ${{ secrets.HF_ACCESS_TOKEN }}
          NEWS_API_KEY: ${{ secrets.NEWS_API_KEY }}

      - name: Ingest new data
        run: |
          echo "Ingesting new data from all sources..."

          # Run all ingest scripts (they should be idempotent)
          python ingest/ingest_yfinance.py || echo "YFinance ingest had issues"
          python ingest/ingest_news.py || echo "News ingest had issues"
          python ingest/ingest_reddit.py || echo "Reddit ingest had issues"
          python ingest/ingest_sec.py || echo "SEC ingest had issues"
          python ingest/ingest_github_hf.py || echo "GitHub/HF ingest had issues"
        continue-on-error: true
        env:
          SEC_API_KEY: ${{ secrets.SEC_API_KEY }}
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
          GITHUB_TOKEN: ${{ secrets.GH_PAT }}
          HF_ACCESS_TOKEN: ${{ secrets.HF_ACCESS_TOKEN }}
          NEWS_API_KEY: ${{ secrets.NEWS_API_KEY }}

      - name: Load new data to warehouse
        run: |
          echo "Loading new staging data to warehouse..."
          python setup/load_warehouse.py

      - name: Compute features for new data
        run: |
          echo "Computing features..."
          python setup/compute_features.py

      - name: Run incremental index update
        run: |
          echo "Running incremental update (only new data points)..."
          python setup/incremental_update.py

      - name: Detect bubble patterns
        run: |
          echo "Running bubble detection..."
          python setup/detect_bubble.py
        continue-on-error: true

      - name: Check for data changes
        id: check-updates
        run: |
          # Check if database was modified
          if [ -f "data/warehouse/ai_bubble.duckdb" ]; then
            # Count size change
            SIZE=$(stat -f%z "data/warehouse/ai_bubble.duckdb" 2>/dev/null || stat -c%s "data/warehouse/ai_bubble.duckdb" 2>/dev/null || echo "0")
            echo "size=$SIZE" >> $GITHUB_OUTPUT
            
            # Check git status
            CHANGED=$(git status --porcelain data/warehouse/ai_bubble.duckdb | wc -l)
            echo "changed=$CHANGED" >> $GITHUB_OUTPUT
            
            echo "Database size: $SIZE bytes"
            echo "Changed files: $CHANGED"
          else
            echo "changed=0" >> $GITHUB_OUTPUT
          fi

      - name: Commit and push database
        if: steps.check-updates.outputs.changed != '0'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Configure git
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'

          # Set the remote URL using the built-in GITHUB_TOKEN
          git remote set-url origin "https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git"

          # Stash any untracked changes to avoid conflicts
          git stash --include-untracked || true

          # Pull latest changes first to avoid conflicts
          echo "Pulling latest changes from remote..."
          git pull origin main --rebase || git pull origin main || true

          # Pop stash if it exists
          git stash pop || true

          # Stage database file
          git add data/warehouse/ai_bubble.duckdb

          # Also add any new raw/staging files (optional)
          git add data/raw/ data/staging/ 2>/dev/null || true

          # Check if there are actually changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit after pulling latest"
            exit 0
          fi

          # Create commit
          TIMESTAMP=$(date +'%Y-%m-%d %H:%M UTC')
          git commit -m "üìä Data update: $TIMESTAMP" \
                     -m "Updated database with latest data" \
                     -m "Size: ${{ steps.check-updates.outputs.size }} bytes"

          # Push changes with retry logic
          MAX_RETRIES=5
          RETRY_COUNT=0
          RETRY_DELAY=10

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            echo "Push attempt $((RETRY_COUNT + 1)) of $MAX_RETRIES..."
            
            if git push origin main; then
              echo "‚úÖ Successfully pushed changes"
              exit 0
            fi
            
            RETRY_COUNT=$((RETRY_COUNT + 1))
            
            if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
              echo "‚ö†Ô∏è Push failed, waiting ${RETRY_DELAY}s before retry..."
              sleep $RETRY_DELAY
              
              # Pull and rebase before retrying
              echo "Pulling latest changes..."
              git pull origin main --rebase || {
                echo "Rebase failed, trying merge..."
                git rebase --abort 2>/dev/null || true
                git pull origin main
              }
            fi
          done

          echo "‚ùå Failed to push after $MAX_RETRIES attempts"
          exit 1

      - name: Generate summary
        if: always()
        run: |
          echo "## Data Update Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp:** $(date +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY

          if [ -f "data/warehouse/ai_bubble.duckdb" ]; then
            # Get database stats using Python
            python3 << 'EOF' >> $GITHUB_STEP_SUMMARY
          import duckdb
          import os
          try:
              conn = duckdb.connect('data/warehouse/ai_bubble.duckdb', read_only=True)
              
              # Get data points
              result = conn.execute("SELECT MIN(date) as first, MAX(date) as last, COUNT(*) as total FROM bubble_metrics").fetchone()
              if result:
                  print(f"- **Date Range:** {result[0]} to {result[1]}")
                  print(f"- **Total Data Points:** {result[2]}")
              
              # Get latest values
              latest = conn.execute("SELECT hype_index, reality_index, hype_reality_gap FROM bubble_metrics ORDER BY date DESC LIMIT 1").fetchone()
              if latest:
                  print(f"- **Latest Hype Index:** {latest[0]:.4f}")
                  print(f"- **Latest Reality Index:** {latest[1]:.4f}")
                  print(f"- **Hype-Reality Gap:** {latest[2]:.4f}")
              
              conn.close()
          except Exception as e:
              print(f"- **Error getting stats:** {e}")
          EOF
          fi

      - name: Create issue on failure
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            const title = '‚ö†Ô∏è Data Update Failed';
            const body = `The incremental data update workflow failed at ${new Date().toISOString()}.
                  
            **Workflow Run:** [View Details](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})
                  
            Please check the logs and investigate the issue.
                  
            Common issues:
            - API rate limits
            - Network connectivity
            - Database corruption
            - Missing dependencies
            - Git push conflicts (check for concurrent workflow runs)
                  `;
                  
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title,
              body,
              labels: ['automated', 'data-update-failed', 'bug']
            });